#!/usr/bin/python3

import subprocess
import sys
import os
import shutil
import tempfile


def available(command):
    return shutil.which(command) is not None


def select_container_manager():
    if available("podman"):
        return "podman"
    elif available("docker"):
        return "docker"

    return "podman"


def image_available(conman, image_name):
    images = conman.copy()
    images.extend(["images", "-q", image_name])
    result = subprocess.run(images,
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    return bool(result.stdout.strip())


def wcurl(url):
    wcurl_cmd = ["curl", "--globoff", "--location", "--proto-default",
                 "https", "--remote-time", "--retry", "10", "--retry-max-time", "10", url]
    result = subprocess.run(
        wcurl_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    return result.stdout.decode('utf-8')


def check_if_in_hf_db(conman, conman_run, vol, image_name):
    host = "raw.githubusercontent.com"
    url = f"https://{host}/ericcurtin/podman-llm/main/hf-db/{image_name}"
    if not image_available(conman, image_name):
        image_data = wcurl(url)
        if image_data:
            hf_repo = [line.split()[1] for line in image_data.splitlines()
                       if line.startswith("hf-repo")][0]
            model = [line.split()[1] for line in image_data.splitlines()
                     if line.startswith("model")][0]
            containerfile = (
                f"FROM quay.io/podman-llm/podman-llm:latest\n"
                f"RUN huggingface-cli download {hf_repo} {model}\n"
                f"RUN ln -s $(huggingface-cli download {
                    hf_repo} {model}) /models/{model}\n"
                f"LABEL MODEL=/models/{model}"
            )
            with tempfile.NamedTemporaryFile(mode='w', delete=False) as tmpfile:
                tmpfile.write(containerfile)
            subprocess.run([conman, "build", vol, "-t",
                           image_name, tmpfile.name])


def get_model(conman, image_name):
    inspect = conman.copy()
    inspect.extend(
        ["inspect", "-f", '{{ index .Config.Labels "MODEL"}}', image_name])
    result = subprocess.run(
        inspect, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    return result.stdout.decode('utf-8').strip()


def get_dangling_images(conman):
    result = subprocess.run([conman, "images", "--filter", "dangling=true",
                            "-q", "--no-trunc"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    return result.stdout.decode('utf-8').splitlines()


def rm_dir(path):
    return os.path.dirname(path)


def get_model_dir(conman_run, image_name, model):
    result = subprocess.run([*conman_run, image_name, "readlink",
                            "-f", model], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    return rm_dir(rm_dir(result.stdout.decode('utf-8').strip()))


def add_dri(conman_run):
    if os.path.exists("/dev/dri"):
        conman_run.extend(["--device", "/dev/dri"])


def run_prep(conman):
    host_hf = os.path.expanduser("~/.cache/huggingface/")
    os.makedirs(host_hf, exist_ok=True)
    vol = f"-v{host_hf}:/root/.cache/huggingface/:z"
    conman_run = conman.copy()
    conman_run.extend([
        "run", "--rm", "-it", "--security-opt=label=disable",
        f"-v{os.path.expanduser('~')}:{os.path.expanduser('~')
                                       }", "-v/tmp:/tmp", vol
    ])

    ngl = False
    if os.path.exists("/proc/driver/nvidia/gpus") or available("nvidia-smi"):
        conman_run.extend(["--gpus=all", "--device", "nvidia.com/gpu=all"])
    elif os.path.exists("/dev/kfd"):
        for i in os.listdir('/sys/bus/pci/devices/'):
            if os.path.exists(f'/sys/bus/pci/devices/{i}/mem_info_vram_total') and int(open(f'/sys/bus/pci/devices/{i}/mem_info_vram_total').read()) > 600000000:
                conman_run.extend(["--device", "/dev/kfd"])
                add_dri(conman_run)
                ngl = True
                break
    elif os.uname().machine == "aarch64":
        add_dri(conman_run)
        ngl = True

    return conman_run, vol, ngl


def rm_cli(conman, conman_run, image_name):
    model = get_model(conman, image_name)
    # Completion of the delete directory logic is needed here
    subprocess.run([conman, "rmi", "-f", image_name])
    dangling_images = get_dangling_images(conman)
    if dangling_images:
        subprocess.run([conman, "rmi", "-f"] + dangling_images)


def build_cli(conman, image_name):
    conman_run, vol, ngl = run_prep(conman)
    subprocess.run([conman, "build", vol, "-t", image_name, "."])


def serve_cli(conman, args):
    if len(args) < 1:
        serve_usage()

    dryrun = False
    image_name = None

    while args:
        arg = args.pop(0)
        if arg in ["-d", "--dryrun"]:
            dryrun = True
        elif arg.startswith("-"):
            serve_usage()
        else:
            image_name = arg

    if image_name is None:
        serve_usage()

    conman_run, vol, ngl = run_prep(conman)
    check_if_in_hf_db(conman, conman_run, vol, image_name)
    model = get_model(conman, image_name)
    conman_run.extend(["-p", f"{os.getenv('PODMAN_LLM_HOST', '8080')}:8080", image_name, "llama-server", "-m", model])
    if dryrun:
        print(" ".join(conman_run))
        return

    subprocess.run(conman_run)


def get_llm_store():
    if os.geteuid() == 0:
        return "/var/lib/podman-llm/storage"
    else:
        return os.path.expanduser("~/.local/share/podman-llm/storage")


def pull_cli(conman, image_name):
    conman_run, vol, ngl = run_prep(conman)
    check_if_in_hf_db(conman, conman_run, vol, image_name)


def serve_usage():
    print(
        "Usage:\n"
        f"  {os.path.basename(__file__)} serve MODEL\n\n"
        "Aliases:\n"
        "  serve, start\n\n"
        "Environment Variables:\n"
        "  PODMAN_LLM_HOST  The host:port to bind to (default \"0.0.0.0:8080\")"
    )
    sys.exit(1)


def run_usage():
    print("Usage:\n"
          f"  {os.path.basename(__file__)} run MODEL\n")
    sys.exit(1)


def run_cli(conman, args):
    if len(args) < 1:
        run_usage()

    dryrun = False
    image_name = None
    while args:
        arg = args.pop(0)
        if arg in ["-d", "--dryrun"]:
            dryrun = True
        elif arg.startswith("-"):
            run_usage()
        else:
            image_name = arg

    if image_name is None:
        run_usage()

    conman_run, vol, ngl = run_prep(conman)
    check_if_in_hf_db(conman, conman_run, vol, image_name)
    model = get_model(conman, image_name)
    conman_run.extend([image_name, "llama-main", "-m",
                      model, "--log-disable", "--instruct"])
    if ngl:
        conman_run.extend(["-ngl", "999"])

    if dryrun:
        print(" ".join(map(str, conman_run)))
        return

    subprocess.run(conman_run)


def conman_cli(conman, args):
    subprocess.run([conman, "--root", get_llm_store()] + args)


def usage():
    print(
        "Usage:\n"
        f"  {os.path.basename(__file__)} COMMAND\n\n"
        "Commands:\n"
        "  run MODEL        Run a model\n"
        "  pull MODEL       Pull a model\n"
        "  serve MODEL      Serve a model\n"
        "  list             List models\n"
        "  rm MODEL         Remove a model\n"
        "  build MODEL      Build a model from a Containerfile")
    sys.exit(1)


def main(args):
    conman = [select_container_manager(), "--root", get_llm_store()]
    if len(args) < 1:
        usage()

    command = args.pop(0)
    if command == "run":
        run_cli(conman, args)
    elif command == "pull":
        pull_cli(conman, args[0])
    elif command in ["serve", "start"]:
        serve_cli(conman, args)
    elif command in ["podman", "docker"]:
        conman_cli(conman, args)
    elif command in ["list", "ls"]:
        subprocess.run([conman, "images", "-flabel=MODEL"])
    elif command == "rm":
        # `None` is a placeholder for conman_run which is not used in rm_cli
        rm_cli(conman, None, args[0])
    elif command == "build":
        build_cli(conman, args[0])
    else:
        usage()


if __name__ == "__main__":
    main(sys.argv[1:])
